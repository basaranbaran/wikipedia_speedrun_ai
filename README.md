# ğŸï¸ Wikipedia Speedrun AI (Local LLM & SBERT)

**[TÃ¼rkÃ§e DokÃ¼mantasyon iÃ§in tÄ±klayÄ±n](#tÃ¼rkÃ§e-dokÃ¼mantasyon)** | **[ğŸ“– English Documentation](#-overview)** | **[ğŸ“Š Video KarÅŸÄ±laÅŸtÄ±rmasÄ±](COMPARISON.md)**

A Tool-Assisted Speedrun (TAS) bot for the Wikipedia Game that uses local AI models to navigate from a random Wikipedia page to a target page by clicking only links, without going back.

This project runs entirely **offline** using your GPU, combining **Semantic Search (SBERT)** for filtering and **Large Language Models (Llama 3.1 via Ollama)** for logical reasoning.

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Methodologies Compared](#-methodologies-compared)
- [Prerequisites](#ï¸-prerequisites)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [Troubleshooting](#-troubleshooting)
- [Video Comparison](COMPARISON.md)

## ğŸ¯ Overview

The Wikipedia Game is a challenge where you start at a random Wikipedia article and try to reach a target article by only clicking links. This project automates this process using three different AI-powered approaches, comparing their performance across various scenarios.

**Key Features:**
- ğŸš€ Fully offline operation (no API keys required)
- ğŸ§  Hybrid AI approach combining semantic search and LLM reasoning
- âš¡ Multiple algorithms for different speed/accuracy trade-offs
- ğŸ“Š Comparative performance analysis
- ğŸ® Pre-configured challenging scenarios

## ğŸ§  Methodologies Compared

The project implements and compares three different algorithmic approaches:

### 1. Hybrid Standard (The Champion ğŸ†)
**File:** `method_1.py`

- **Logic:** Uses SBERT to find the top 10 mathematically closest links, then asks **Llama 3.1** to pick the most logical one.
- **Pros:** 
  - Fast (3-5s per step)
  - Context-aware
  - Avoids common semantic traps (e.g., "White House" vs "White House painting")
- **Cons:** Slightly slower than pure vector-based approach

### 2. Pure SBERT (Speed Demon âš¡)
**File:** `method_2.py`

- **Logic:** Calculates Cosine Similarity between the target vector and link vectors. Picks the #1 mathematically closest link immediately.
- **Pros:** 
  - Extremely fast (<1s per step)
  - No LLM overhead
- **Cons:** 
  - Context-blind
  - Easily trapped by homonyms (words that look alike but mean different things)

### 3. Chain-of-Thought (The Professor)
**File:** `method_3.py`

- **Logic:** Filters top 5 links, then asks Llama 3.1 to **explain its reasoning** before selecting a link.
- **Pros:** 
  - Highest potential accuracy for complex, indirect connections
  - Transparent decision-making process
- **Cons:** 
  - Slowest due to token generation for explanations
  - Often "overthinks" simple paths

## ğŸ› ï¸ Prerequisites

Before you begin, ensure you have:

- **Python 3.10+** installed
- **Ollama** installed and running (`ollama serve`)
- **Llama 3.1** model pulled (`ollama pull llama3.1`)
- **NVIDIA GPU** (Recommended: RTX 3060 or better) with CUDA support
  - CPU mode is supported but significantly slower

### Installing Ollama

1. Download Ollama from [ollama.ai](https://ollama.ai)
2. Install and start the service:
   ```bash
   ollama serve
   ```
3. Pull the required model:
   ```bash
   ollama pull llama3.1
   ```

## ğŸ“¦ Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd wikipedia-speedrun
   ```

2. **Create a virtual environment:**
   ```bash
   python -m venv venv
   ```
   
   **Windows:**
   ```bash
   .\venv\Scripts\activate
   ```
   
   **Linux/Mac:**
   ```bash
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install PyTorch with CUDA support (if using GPU):**
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
   ```
   
   *Note: The requirements.txt includes torch, but for CUDA support, you may need to install it separately as shown above.*

## ğŸš€ Usage

### Running the Comparison

Run the main comparison script to see all three methods compete:

```bash
python comparison.py
```

This will:
- Test all three methods on predefined scenarios
- Display real-time progress for each method
- Show a comparison table at the end with results

### Example Output

```
ğŸ† WIKIPEDIA SPEEDRUN ALGORITHM BATTLE ğŸ†
============================================================

ğŸŒ SCENARIO: Potato -> Barack Obama
------------------------------------------------------------
â–¶ï¸  AGENT: Hybrid (Method 1) Running...
      ğŸ“ Start: [Potato]
      ğŸ‘‰ [Step 1] Potato --> 'Agriculture'
      ğŸ‘‰ [Step 2] Agriculture --> 'United States'
      ...
ğŸ RESULT: SUCCESS! (45.23 seconds)
```

### Custom Scenarios

To test your own scenarios, edit `comparison.py` and modify the `SCENARIOS` list:

```python
SCENARIOS = [
    {
        "start": "https://en.wikipedia.org/wiki/YourStartPage",
        "target": "Your Target Topic",
        "keywords": "target keywords for semantic search"
    },
]
```

## ğŸ“ Project Structure

```
wikipedia-speedrun/
â”œâ”€â”€ comparison.py      # Main comparison script that runs all methods
â”œâ”€â”€ method_1.py        # Hybrid Standard approach (SBERT + LLM)
â”œâ”€â”€ method_2.py        # Pure SBERT approach (vector similarity only)
â”œâ”€â”€ method_3.py        # Chain-of-Thought approach (reasoning + LLM)
â”œâ”€â”€ utils.py           # Shared utilities (link extraction, SBERT matching)
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ README.md          # This file
â””â”€â”€ venv/              # Virtual environment (not in repo)
```

## ğŸ”§ How It Works

1. **Link Extraction**: Fetches Wikipedia page, extracts valid article links, filters non-article pages
2. **Semantic Filtering**: Uses SBERT (`all-MiniLM-L6-v2`) to encode target keywords and link titles, calculates cosine similarity, returns top K matches
3. **Decision Making**: 
   - Method 1: LLM selects from top 10 SBERT matches
   - Method 2: Direct selection of top 1 SBERT match
   - Method 3: LLM reasons through top 5 matches with explanations
4. **Navigation**: Follows selected link, tracks visited pages, checks for shortcuts, stops on success/dead end/timeout (15 steps max)

## ğŸ› Troubleshooting

**Ollama Issues:** Ensure `ollama serve` is running, check model with `ollama list`, verify PATH

**GPU Issues:** Verify CUDA: `python -c "import torch; print(torch.cuda.is_available())"`, reinstall PyTorch with CUDA support, check with `nvidia-smi`

**Slow Performance:** Ensure GPU usage, reduce `top_k` values, use Method 2 for speed, or try smaller LLM model

**Model Download:** Check internet (first run only), model caches after download, verify `sentence-transformers` installed

**Wikipedia Blocking:** Code includes User-Agent header, add delays if needed, consider Wikipedia API

## ğŸ“Š Performance Notes

- **Method 1 (Hybrid)**: Best balance of speed and accuracy
- **Method 2 (Pure SBERT)**: Fastest, good for simple connections
- **Method 3 (CoT)**: Slowest but most thorough for complex paths

Typical performance on RTX 3060:
- Method 1: 3-5 seconds per step
- Method 2: <1 second per step
- Method 3: 10-15 seconds per step

## ğŸ§ª Test Results Analysis

Based on real test runs with 3 challenging scenarios:

### Overall Performance Summary

| Method | Success Rate | Avg Time | Avg Steps | Best For |
|--------|-------------|----------|-----------|----------|
| **Method 1 (Hybrid)** | **100% (3/3)** | **5.5s** | **5.3 steps** | **General use - most reliable** |
| Method 2 (SBERT) | 67% (2/3) | 6.1s | 10 steps | Simple, direct connections |
| Method 3 (CoT) | 100% (3/3) | 13.6s | 7 steps | Complex, indirect paths |

### Key Findings

âœ… **Method 1 (Hybrid) - The Winner:**
- **100% success rate** across all scenarios
- Consistent performance (3-8 seconds)
- Efficient pathfinding (4-7 steps)
- Successfully avoided semantic traps that caught Method 2

âš ï¸ **Method 2 (SBERT) - Speed Demon with Risks:**
- Fastest when it works (2.5-9.5 seconds)
- **Failed on complex scenario** (Hallucinogenic fish â†’ Andrej Karpathy)
- Got trapped by homonyms:
  - "White House at Night" (painting) instead of "White House" (building)
  - "Tesla" (film) instead of "Tesla" (company/scientist)
- Validates README warning: "Easily trapped by homonyms"

ğŸ¤” **Method 3 (CoT) - The Reliable Thinker:**
- **100% success rate** - never failed
- Slowest overall (5-29 seconds)
- Most thorough reasoning for complex paths
- Sometimes "overthinks" simple paths (took longer than Method 1 in Scenario 2)

### Real-World Example: Potato â†’ Barack Obama

- **Method 1**: Potato â†’ United States â†’ African American â†’ **Barack Obama** (4 steps, 7.6s)
- **Method 2**: Potato â†’ Anton Mauve â†’ White House at Night â†’ ... (14 steps, 9.6s) - *Got distracted by "White House" painting*
- **Method 3**: Potato â†’ United States â†’ President â†’ **Barack Obama** (4 steps, 5.3s) - *Most direct path*

### Conclusion

**Method 1 (Hybrid)** proves to be the optimal choice for general use, combining the speed of semantic search with the intelligence of LLM reasoning. It successfully avoids the pitfalls that trap pure vector-based approaches while maintaining reasonable speed.

---

<a id="tÃ¼rkÃ§e-dokÃ¼mantasyon"></a>
# ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e DokÃ¼mantasyon

## ğŸ¯ Genel BakÄ±ÅŸ

Wikipedia Oyunu, rastgele bir Wikipedia makalesinden baÅŸlayÄ±p sadece linklere tÄ±klayarak hedef makaleye ulaÅŸma oyunudur. Bu proje, bu sÃ¼reci otomatikleÅŸtirir ve Ã¼Ã§ farklÄ± AI destekli yaklaÅŸÄ±mÄ± karÅŸÄ±laÅŸtÄ±rÄ±r.

**Ã–zellikler:**
- ğŸš€ Tamamen offline Ã§alÄ±ÅŸma (API anahtarÄ± gerekmez)
- ğŸ§  Semantik arama ve LLM akÄ±l yÃ¼rÃ¼tmeyi birleÅŸtiren hibrit AI yaklaÅŸÄ±mÄ±
- âš¡ FarklÄ± hÄ±z/doÄŸruluk dengesi iÃ§in Ã§oklu algoritmalar
- ğŸ“Š KarÅŸÄ±laÅŸtÄ±rmalÄ± performans analizi
- ğŸ® Ã–nceden yapÄ±landÄ±rÄ±lmÄ±ÅŸ zorlu senaryolar

## ğŸ§  KarÅŸÄ±laÅŸtÄ±rÄ±lan Metodolojiler

### 1. Hibrit Standart (Åampiyon ğŸ†)
**Dosya:** `method_1.py`

- **MantÄ±k:** SBERT ile en yakÄ±n 10 linki bulur, sonra **Llama 3.1**'den en mantÄ±klÄ± olanÄ± seÃ§mesini ister
- **ArtÄ±larÄ±:** HÄ±zlÄ± (adÄ±m baÅŸÄ±na 3-5s), baÄŸlam farkÄ±ndalÄ±ÄŸÄ±, semantik tuzaklardan kaÃ§Ä±nÄ±r
- **Eksileri:** Saf vektÃ¶r yaklaÅŸÄ±mÄ±ndan biraz daha yavaÅŸ

### 2. Saf SBERT (HÄ±z CanavarÄ± âš¡)
**Dosya:** `method_2.py`

- **MantÄ±k:** Hedef vektÃ¶r ile link vektÃ¶rleri arasÄ±ndaki kosinÃ¼s benzerliÄŸini hesaplar, en yakÄ±n linki anÄ±nda seÃ§er
- **ArtÄ±larÄ±:** Ã‡ok hÄ±zlÄ± (adÄ±m baÅŸÄ±na <1s), LLM yÃ¼kÃ¼ yok
- **Eksileri:** BaÄŸlam kÃ¶rÃ¼, homonimlerle kolayca tuzaÄŸa dÃ¼ÅŸer

### 3. DÃ¼ÅŸÃ¼nce Zinciri (ProfesÃ¶r ğŸ¤”)
**Dosya:** `method_3.py`

- **MantÄ±k:** En iyi 5 linki filtreler, sonra Llama 3.1'den **akÄ±l yÃ¼rÃ¼tmesini aÃ§Ä±klamasÄ±nÄ±** ister
- **ArtÄ±larÄ±:** KarmaÅŸÄ±k, dolaylÄ± baÄŸlantÄ±lar iÃ§in en yÃ¼ksek doÄŸruluk potansiyeli
- **Eksileri:** AÃ§Ä±klamalar iÃ§in token Ã¼retimi nedeniyle en yavaÅŸ, basit yollarÄ± "fazla dÃ¼ÅŸÃ¼nÃ¼r"

## ğŸ› ï¸ Gereksinimler

- **Python 3.10+** yÃ¼klÃ¼
- **Ollama** yÃ¼klÃ¼ ve Ã§alÄ±ÅŸÄ±yor (`ollama serve`)
- **Llama 3.1** modeli indirilmiÅŸ (`ollama pull llama3.1`)
- **NVIDIA GPU** (Ã–nerilen: RTX 3060 veya daha iyi) CUDA desteÄŸi ile
  - CPU modu desteklenir ancak Ã§ok daha yavaÅŸtÄ±r

### Ollama Kurulumu

1. [ollama.ai](https://ollama.ai) adresinden Ollama'yÄ± indirin
2. Servisi baÅŸlatÄ±n:
   ```bash
   ollama serve
   ```
3. Gerekli modeli indirin:
   ```bash
   ollama pull llama3.1
   ```

## ğŸ“¦ Kurulum

1. **Repository'yi klonlayÄ±n:**
   ```bash
   git clone <repository-url>
   cd wikipedia-speedrun
   ```

2. **Sanal ortam oluÅŸturun:**
   ```bash
   python -m venv venv
   ```
   
   **Windows:**
   ```bash
   .\venv\Scripts\activate
   ```
   
   **Linux/Mac:**
   ```bash
   source venv/bin/activate
   ```

3. **BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:**
   ```bash
   pip install -r requirements.txt
   ```

4. **GPU iÃ§in PyTorch CUDA desteÄŸi (opsiyonel):**
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
   ```

## ğŸš€ KullanÄ±m

### KarÅŸÄ±laÅŸtÄ±rmayÄ± Ã‡alÄ±ÅŸtÄ±rma

ÃœÃ§ yÃ¶ntemin rekabetini gÃ¶rmek iÃ§in ana karÅŸÄ±laÅŸtÄ±rma scriptini Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python comparison.py
```

Bu script:
- Ã–nceden tanÄ±mlanmÄ±ÅŸ senaryolarda Ã¼Ã§ yÃ¶ntemi test eder
- Her yÃ¶ntem iÃ§in gerÃ§ek zamanlÄ± ilerlemeyi gÃ¶sterir
- Sonunda sonuÃ§larla bir karÅŸÄ±laÅŸtÄ±rma tablosu gÃ¶sterir

### Ã–rnek Ã‡Ä±ktÄ±

```
ğŸ† WIKIPEDIA SPEEDRUN ALGORITHM BATTLE ğŸ†
============================================================

ğŸŒ SCENARIO: Potato -> Barack Obama
------------------------------------------------------------
â–¶ï¸  AGENT: Hybrid (Method 1) Running...
      ğŸ“ Start: [Potato]
      ğŸ‘‰ [Step 1] Potato --> 'Agriculture'
      ğŸ‘‰ [Step 2] Agriculture --> 'United States'
      ...
ğŸ RESULT: SUCCESS! (45.23 seconds)
```

### Ã–zel Senaryolar

Kendi senaryolarÄ±nÄ±zÄ± test etmek iÃ§in `comparison.py` dosyasÄ±nÄ± dÃ¼zenleyin ve `SCENARIOS` listesini deÄŸiÅŸtirin:

```python
SCENARIOS = [
    {
        "start": "https://en.wikipedia.org/wiki/BaÅŸlangÄ±Ã§SayfasÄ±",
        "target": "Hedef Konu",
        "keywords": "hedef anahtar kelimeler semantik arama iÃ§in"
    },
]
```

## ğŸ“ Proje YapÄ±sÄ±

```
wikipedia-speedrun/
â”œâ”€â”€ comparison.py      # TÃ¼m yÃ¶ntemleri Ã§alÄ±ÅŸtÄ±ran ana karÅŸÄ±laÅŸtÄ±rma scripti
â”œâ”€â”€ method_1.py        # Hibrit Standart yaklaÅŸÄ±m (SBERT + LLM)
â”œâ”€â”€ method_2.py        # Saf SBERT yaklaÅŸÄ±mÄ± (sadece vektÃ¶r benzerliÄŸi)
â”œâ”€â”€ method_3.py        # DÃ¼ÅŸÃ¼nce Zinciri yaklaÅŸÄ±mÄ± (akÄ±l yÃ¼rÃ¼tme + LLM)
â”œâ”€â”€ utils.py           # Ortak yardÄ±mcÄ± fonksiyonlar (link Ã§Ä±karma, SBERT eÅŸleÅŸtirme)
â”œâ”€â”€ requirements.txt   # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ README.md          # Bu dosya
â””â”€â”€ venv/              # Sanal ortam (repo'da deÄŸil)
```

## ğŸ”§ NasÄ±l Ã‡alÄ±ÅŸÄ±r

1. **Link Ã‡Ä±karma**: Wikipedia sayfasÄ±nÄ± getirir, geÃ§erli makale linklerini Ã§Ä±karÄ±r, makale olmayan sayfalarÄ± filtreler
2. **Semantik Filtreleme**: SBERT (`all-MiniLM-L6-v2`) kullanarak hedef anahtar kelimeleri ve link baÅŸlÄ±klarÄ±nÄ± kodlar, kosinÃ¼s benzerliÄŸi hesaplar, en iyi K eÅŸleÅŸmeyi dÃ¶ndÃ¼rÃ¼r
3. **Karar Verme**: 
   - YÃ¶ntem 1: LLM en iyi 10 SBERT eÅŸleÅŸmesinden seÃ§er
   - YÃ¶ntem 2: En iyi 1 SBERT eÅŸleÅŸmesini doÄŸrudan seÃ§er
   - YÃ¶ntem 3: LLM en iyi 5 eÅŸleÅŸmeyi akÄ±l yÃ¼rÃ¼terek aÃ§Ä±klamalarla seÃ§er
4. **Navigasyon**: SeÃ§ilen linki takip eder, ziyaret edilen sayfalarÄ± takip eder, kÄ±sayollarÄ± kontrol eder, baÅŸarÄ±/Ã§Ä±kmaz/zaman aÅŸÄ±mÄ±nda durur (maksimum 15 adÄ±m)

## ğŸ› Sorun Giderme

**Ollama SorunlarÄ±:** `ollama serve` Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun, modeli `ollama list` ile kontrol edin, PATH'i doÄŸrulayÄ±n

**GPU SorunlarÄ±:** CUDA'yÄ± doÄŸrulayÄ±n: `python -c "import torch; print(torch.cuda.is_available())"`, PyTorch'u CUDA desteÄŸi ile yeniden yÃ¼kleyin, `nvidia-smi` ile kontrol edin

**YavaÅŸ Performans:** GPU kullanÄ±mÄ±nÄ± saÄŸlayÄ±n, `top_k` deÄŸerlerini azaltÄ±n, hÄ±z iÃ§in YÃ¶ntem 2'yi kullanÄ±n veya daha kÃ¼Ã§Ã¼k LLM modeli deneyin

**Model Ä°ndirme:** Ä°nternet baÄŸlantÄ±sÄ±nÄ± kontrol edin (sadece ilk Ã§alÄ±ÅŸtÄ±rmada), model indirmeden sonra Ã¶nbelleÄŸe alÄ±nÄ±r, `sentence-transformers` yÃ¼klÃ¼ olduÄŸunu doÄŸrulayÄ±n

**Wikipedia Engelleme:** Kod User-Agent baÅŸlÄ±ÄŸÄ± iÃ§erir, gerekirse gecikmeler ekleyin, Wikipedia API'sini dÃ¼ÅŸÃ¼nÃ¼n

## ğŸ“Š Performans NotlarÄ±

- **YÃ¶ntem 1 (Hibrit)**: HÄ±z ve doÄŸruluk dengesi en iyi
- **YÃ¶ntem 2 (Saf SBERT)**: En hÄ±zlÄ±, basit baÄŸlantÄ±lar iÃ§in iyi
- **YÃ¶ntem 3 (DÃ¼ÅŸÃ¼nce Zinciri)**: En yavaÅŸ ama karmaÅŸÄ±k yollar iÃ§in en kapsamlÄ±

RTX 3060'ta tipik performans:
- YÃ¶ntem 1: AdÄ±m baÅŸÄ±na 3-5 saniye
- YÃ¶ntem 2: AdÄ±m baÅŸÄ±na <1 saniye
- YÃ¶ntem 3: AdÄ±m baÅŸÄ±na 10-15 saniye

## ğŸ§ª Test SonuÃ§larÄ± Analizi

3 zorlu senaryo ile gerÃ§ek test sonuÃ§larÄ±na gÃ¶re:

### Genel Performans Ã–zeti

| YÃ¶ntem | BaÅŸarÄ± OranÄ± | Ortalama SÃ¼re | Ortalama AdÄ±m | En Ä°yi KullanÄ±m |
|--------|-------------|---------------|---------------|-----------------|
| **YÃ¶ntem 1 (Hibrit)** | **%100 (3/3)** | **5.5s** | **5.3 adÄ±m** | **Genel kullanÄ±m - en gÃ¼venilir** |
| YÃ¶ntem 2 (SBERT) | %67 (2/3) | 6.1s | 10 adÄ±m | Basit, doÄŸrudan baÄŸlantÄ±lar |
| YÃ¶ntem 3 (DÃ¼ÅŸÃ¼nce Zinciri) | %100 (3/3) | 13.6s | 7 adÄ±m | KarmaÅŸÄ±k, dolaylÄ± yollar |

### Ã–nemli Bulgular

âœ… **YÃ¶ntem 1 (Hibrit) - Kazanan:**
- TÃ¼m senaryolarda **%100 baÅŸarÄ± oranÄ±**
- TutarlÄ± performans (3-8 saniye)
- Verimli yol bulma (4-7 adÄ±m)
- YÃ¶ntem 2'nin takÄ±ldÄ±ÄŸÄ± semantik tuzaklardan baÅŸarÄ±yla kaÃ§Ä±ndÄ±

âš ï¸ **YÃ¶ntem 2 (SBERT) - HÄ±z CanavarÄ± ama Riskli:**
- Ã‡alÄ±ÅŸtÄ±ÄŸÄ±nda en hÄ±zlÄ± (2.5-9.5 saniye)
- **KarmaÅŸÄ±k senaryoda baÅŸarÄ±sÄ±z** (Hallucinogenic fish â†’ Andrej Karpathy)
- Homonimlerle tuzaÄŸa dÃ¼ÅŸtÃ¼:
  - "White House at Night" (resim) yerine "White House" (bina)
  - "Tesla" (film) yerine "Tesla" (ÅŸirket/bilim insanÄ±)
- README uyarÄ±sÄ±nÄ± doÄŸruladÄ±: "Homonimlerle kolayca tuzaÄŸa dÃ¼ÅŸer"

ğŸ¤” **YÃ¶ntem 3 (DÃ¼ÅŸÃ¼nce Zinciri) - GÃ¼venilir DÃ¼ÅŸÃ¼nÃ¼r:**
- **%100 baÅŸarÄ± oranÄ±** - hiÃ§ baÅŸarÄ±sÄ±z olmadÄ±
- En yavaÅŸ genel performans (5-29 saniye)
- KarmaÅŸÄ±k yollar iÃ§in en kapsamlÄ± akÄ±l yÃ¼rÃ¼tme
- Bazen basit yollarÄ± "fazla dÃ¼ÅŸÃ¼nÃ¼r" (Senaryo 2'de YÃ¶ntem 1'den daha uzun sÃ¼rdÃ¼)

### GerÃ§ek DÃ¼nya Ã–rneÄŸi: Potato â†’ Barack Obama

- **YÃ¶ntem 1**: Potato â†’ United States â†’ African American â†’ **Barack Obama** (4 adÄ±m, 7.6s)
- **YÃ¶ntem 2**: Potato â†’ Anton Mauve â†’ White House at Night â†’ ... (14 adÄ±m, 9.6s) - *"White House" resmiyle dikkati daÄŸÄ±ldÄ±*
- **YÃ¶ntem 3**: Potato â†’ United States â†’ President â†’ **Barack Obama** (4 adÄ±m, 5.3s) - *En doÄŸrudan yol*

### SonuÃ§

**YÃ¶ntem 1 (Hibrit)** genel kullanÄ±m iÃ§in optimal seÃ§im olduÄŸunu kanÄ±tladÄ±. Semantik aramanÄ±n hÄ±zÄ±nÄ± LLM akÄ±l yÃ¼rÃ¼tmesinin zekasÄ±yla birleÅŸtiriyor. Saf vektÃ¶r tabanlÄ± yaklaÅŸÄ±mlarÄ±n tuzaÄŸÄ±na dÃ¼ÅŸmeden makul bir hÄ±z koruyor.

## ğŸ“š Referanslar

- [Sentence-BERT](https://www.sbert.net/)
- [Ollama](https://ollama.ai/)
- [Wikipedia Game](https://en.wikipedia.org/wiki/Wikipedia:Wiki_Game)

**Ä°yi Speedrun'lar! ğŸï¸ğŸ’¨**
